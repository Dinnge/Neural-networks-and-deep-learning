# 利用循环神经网络生成唐诗

## 1 循环神经网络

- RNN

  ‌**[RNN](https://www.baidu.com/s?rsv_dl=re_dqa_generate&sa=re_dqa_generate&wd=RNN&rsv_pq=a2319d9f01391f53&oq=RNN和LSTM介绍&rsv_t=98f9ev0VPDfBDL8659IVUQ7yYZC6jJ/6mcwZbDJrMPLL3xlT2itu346vUxde7/MoGu0c7oE&tn=15007414_23_dg&ie=utf-8)（Recurrent Neural Network，循环神经网络）**‌是一种专门用于处理序列数据的神经网络。RNN通过将前一个时间步的输出反馈到当前时间步，使得模型能够“记住”之前的输入信息，从而适合处理时间序列或文本等有序数据。RNN的基本结构包括输入层、隐藏层和输出层，其中隐藏层通过循环连接，能够捕捉序列中的上下文关系‌12。

- LSTM

  **LSTM（Long Short-Term Memory，长短期记忆网络）**‌是为了克服RNN的局限性而设计的。LSTM引入了“记忆单元”和三个“门控机制”：遗忘门、输入门和输出门。这些门控机制使得LSTM能够更好地捕捉长期依赖关系，并通过门控机制有效减轻梯度消失问题‌1。LSTM的核心优势在于其能够更好地处理长期依赖问题，适用于需要长期记忆的应用场景‌2。

  - **LSTM 的基本结构**

    LSTM 的核心是引入了“记忆单元”（Cell State）和三个“门控机制”：

    - **遗忘门（Forget Gate）**：决定需要丢弃哪些信息。
    - **输入门（Input Gate）**：决定需要添加哪些新信息。
    - **输出门（Output Gate）**：决定输出哪些信息作为当前时间步的隐藏状态。

  - **LSTM 的优势**

    - 能够更好地捕捉长期依赖关系。
    - 通过门控机制有效减轻梯度消失问题。

## 2 实验过程

#### **2.1 tensorflow**

- 数据预处理

  **`process_dataset(fileName)`**：

  - 从指定的文本文件中读取诗句，生成以特定标记（`start_token` 和 `end_token`）包围的整数序列。
  - 统计词频，构建词汇表（`word2id` 和 `id2word`），并返回索引化的诗句和序列长度。

  **`poem_dataset()`**：

  - 调用 `process_dataset` 函数获取索引化的实例和词汇表。
  - 创建一个 TensorFlow 数据集，随机打乱数据，按批次填充，并将输入和输出序列进行适当的分割，以便用于模型训练。

- 模型搭建

  **`__init__(self, w2id)`**：

  - 初始化模型，包括词汇大小和嵌入层、RNN 层和全连接层。

  **`call(self, inp_ids)`**：

  - 定义前向传播过程，输入整数序列经过嵌入层、RNN 层和全连接层，返回预测的 logits。

  ![image-20250327233326243](C:\Users\Dy.ming\AppData\Roaming\Typora\typora-user-images\image-20250327233326243.png)

  **`get_next_token(self, x, state)`**：

  - 处理输入的单个令牌（`x`），计算其嵌入，更新 RNN 状态，并返回下一个预测的令牌及更新后的状态。

- 损失函数

  **`compute_loss(logits, labels, seqlen)`**：

  - 使用 `sparse_softmax_cross_entropy_with_logits` 计算模型输出（`logits`）与真实标签（`labels`）之间的损失。
  - 通过 `reduce_avg` 函数根据序列长度（`seqlen`）对损失进行平均，并返回最终的平均损失值。

  **`train_one_step(model, optimizer, x, y, seqlen)`**：

  - 在 `tf.GradientTape` 上下文中执行前向传播，计算模型的输出和损失。
  - 计算梯度并应用于优化器；同时打印当前的损失值和 logits。此函数负责执行一次训练步骤。

  ![image-20250327233344941](C:\Users\Dy.ming\AppData\Roaming\Typora\typora-user-images\image-20250327233344941.png)

  **`train(epoch, model, optimizer, ds)`**：

  - 迭代调用 `train_one_step` 函数进行训练，并在每 500 步打印当前的损失值。此函数用于管理整个训练过程。

- 训练网络模型

  初始化优化器，准备数据集进行训练循环。

- 生成唐诗

  **`gen_sentence()`**：

  - 初始化 RNN 状态为随机值，形状为 `(1, 128)`，表示一个批次的隐藏状态。
  - 使用特定的起始令牌（例如 `'红'`）作为输入的当前令牌，类型为整数。

  **句子生成循环**：

  - 循环 50 次，通过调用模型的 `get_next_token()` 方法获取下一个令牌和更新的状态。
  - 将生成的令牌存储在 `collect` 列表中。

  **返回生成的句子**：

  - 将收集的令牌转换为对应的词，并返回生成的句子。

#### 2.2 pytorch

​	和tensorflow的思路大致相同。

- 数据处理

  **`process_poems1(file_name)`**：

  - 从指定的文件中读取每行诗歌，提取标题和内容。
  - 对内容进行清理，去除不符合条件的诗（如长度不在 5 到 80 之间，或包含特定字符）。
  - 将合适的诗内容加上起始和结束标记，存入 `poems` 列表。
  - 统计所有字的频率，创建字到索引的映射（`word_int_map`），并返回诗的向量表示（`poems_vector`）、字典和字列表。

  **`process_poems2(file_name)`**：

  - 该函数实现与 `process_poems1` 相似的功能，主要区别在于对内容的清理方式与处理逻辑略有不同。
  - 读取的内容去掉了空格和标点符号，处理后存入 `poems` 列表，并进行相同的统计和映射操作。

  **`generate_batch(batch_size, poems_vec, word_to_int)`**：

  - 输入参数包括批次大小 `batch_size`、诗的向量表示 `poems_vec` 和字到整数的映射 `word_to_int`。
  - 计算可以生成的完整批次数量 `n_chunk`，即诗的总数量除以批次大小。

- 模型定义

  **`weights_init(m)`**：

  - 初始化模型权重，适用于线性层。根据 Fan-in 和 Fan-out 计算权重的边界，并用均匀分布初始化权重。

  **`word_embedding` 类**：

  - 定义词嵌入层，初始化一个形状为 `(vocab_length, embedding_dim)` 的随机权重矩阵，并将其赋值给嵌入层。

  **`RNN_model` 类**：

  - 继承自 `nn.Module`，定义了 RNN 模型结构。
  - 包含词嵌入层、LSTM 层（具有两层）、全连接层以及激活函数（LogSoftmax）。
  - 在 `__init__` 方法中设置 LSTM 输入和输出尺寸。

  **`forward(self, sentence, is_test=False)`**：

  - 实现前向传播，输入句子经过词嵌入层转换为嵌入向量。
  - 将嵌入向量输入 LSTM 层，获取输出并进行后续处理。
  - 在测试阶段，仅返回最后一个时间步的输出；在训练阶段，返回整个序列的输出。

![image-20250327233954520](C:\Users\Dy.ming\AppData\Roaming\Typora\typora-user-images\image-20250327233954520.png)

![image-20250327233944992](C:\Users\Dy.ming\AppData\Roaming\Typora\typora-user-images\image-20250327233944992.png)

- 模型训练

  **数据处理**：

  - 使用 `process_poems1` 函数加载数据集，得到诗的向量表示、词到整数的映射和词汇表。

  **模型和优化器初始化**：

  - 设置批次大小 `BATCH_SIZE` 为 100。
  - 创建词嵌入层和 RNN 模型实例。
  - 使用 RMSprop 优化器和负对数似然损失函数（`NLLLoss`）。
  - 如果已有训练好的模型，则加载其状态。

  **训练循环**：

  - 迭代 30 个周期（`epoch`），在每个周期中生成批次数据。
  - 对于每个批次，计算每个样本的损失，并进行反向传播。
  - 使用 `clip_grad_norm` 防止梯度爆炸，执行优化步骤。

- 生成唐诗

  **`pretty_print_poem(poem)`**：

  - 用于格式化打印生成的诗歌，使其更整洁。
  - 遍历输入的 `poem`，去除起始和结束标记（`start_token` 和 `end_token`）。
  - 将诗句按句号（`。`）分割，并打印每个句子，如果句子长度超过 10 个字符，则在末尾加上句号。

  **`gen_poem(begin_word)`**：

  - 用于生成诗歌，输入一个起始字（`begin_word`）。
  - 加载训练好的 RNN 模型，准备词嵌入层。
  - 初始化以 `begin_word` 开头的诗歌。
  - 在循环中，根据当前诗歌生成下一个字，直到遇到结束标记（`end_token`）或诗歌长度超过 30 个字为止。
  - 返回生成的诗歌。

## 3 训练截图

![f36edcb4c2440b010f297bb8187da1c](C:\Users\Dy.ming\Documents\WeChat Files\wxid_dlb50a6t3saj22\FileStorage\Temp\f36edcb4c2440b010f297bb8187da1c.png)

![f2edc2173424ef930360c91017b05ec](C:\Users\Dy.ming\Documents\WeChat Files\wxid_dlb50a6t3saj22\FileStorage\Temp\f2edc2173424ef930360c91017b05ec.png)